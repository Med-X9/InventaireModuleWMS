#!/usr/bin/env python
"""
Test unitaire pour l'API CountingDetail avec 1000 lignes.
Ce script teste la performance et la robustesse de l'API counting-detail
en crÃ©ant et validant 1000 CountingDetail avec diffÃ©rents scÃ©narios.

Usage:
    python test_counting_detail_1000_lines.py

Auteur: Assistant IA
Date: 2024-12-15
"""

import os
import sys
import django
import requests
import json
import time
import random
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import statistics

# Configuration Django
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'project.settings')
django.setup()

from django.contrib.auth import get_user_model
from django.test import TestCase
from django.urls import reverse
from rest_framework.test import APIClient
from rest_framework import status
from apps.inventory.models import Inventory, Counting, CountingDetail, NSerieInventory, Job, JobDetail, Assigment
from apps.masterdata.models import Product, Location, Warehouse, Account
from apps.users.models import User

User = get_user_model()


class CountingDetailPerformanceTest:
    """
    Classe de test de performance pour l'API CountingDetail.
    Teste la crÃ©ation de 1000 CountingDetail avec diffÃ©rents scÃ©narios.
    """
    
    def __init__(self):
        self.api_client = APIClient()
        self.base_url = "http://localhost:8000"
        self.api_base = f"{self.base_url}/mobile/api"
        self.test_user = None
        self.test_token = None
        self.test_data = {
            'accounts': [],
            'warehouses': [],
            'locations': [],
            'products': [],
            'inventories': [],
            'countings': [],
            'assignments': [],
            'jobs': [],
            'job_details': []
        }
        self.performance_metrics = {
            'creation_times': [],
            'response_times': [],
            'success_count': 0,
            'error_count': 0,
            'errors': []
        }
        
    def setUp(self):
        """Configuration initiale des donnÃ©es de test."""
        print("ğŸ”§ Configuration initiale des donnÃ©es de test...")
        
        # CrÃ©er un utilisateur de test
        self.test_user = User.objects.create_user(
            username='test_counting_user',
            email='test@example.com',
            password='testpass123'
        )
        
        # Obtenir le token d'authentification
        self._authenticate()
        
        # CrÃ©er les donnÃ©es de base nÃ©cessaires
        self._create_base_data()
        
        print("âœ… Configuration terminÃ©e")
    
    def _authenticate(self):
        """Authentification et rÃ©cupÃ©ration du token."""
        try:
            # Utiliser l'API JWT pour l'authentification
            auth_data = {
                'username': 'test_counting_user',
                'password': 'testpass123'
            }
            
            response = requests.post(
                f"{self.api_base}/auth/jwt-login/",
                json=auth_data
            )
            
            if response.status_code == 200:
                self.test_token = response.json()['access_token']
                print("âœ… Authentification rÃ©ussie")
            else:
                print(f"âŒ Erreur d'authentification: {response.text}")
                # Fallback: utiliser le client API Django pour les tests
                self.api_client.force_authenticate(user=self.test_user)
                
        except Exception as e:
            print(f"âŒ Erreur d'authentification: {e}")
            # Utiliser le client API Django
            self.api_client.force_authenticate(user=self.test_user)
    
    def _create_base_data(self):
        """CrÃ©e les donnÃ©es de base nÃ©cessaires pour les tests."""
        print("ğŸ“¦ CrÃ©ation des donnÃ©es de base...")
        
        # CrÃ©er des comptes
        for i in range(5):
            account = Account.objects.create(
                name=f"Test Account {i+1}",
                description=f"Compte de test {i+1}"
            )
            self.test_data['accounts'].append(account)
        
        # CrÃ©er des entrepÃ´ts
        for i, account in enumerate(self.test_data['accounts']):
            warehouse = Warehouse.objects.create(
                name=f"Test Warehouse {i+1}",
                account=account
            )
            self.test_data['warehouses'].append(warehouse)
        
        # CrÃ©er des emplacements (200 emplacements)
        location_count = 0
        for warehouse in self.test_data['warehouses']:
            for j in range(40):  # 40 emplacements par entrepÃ´t
                location = Location.objects.create(
                    name=f"LOC-{warehouse.name}-{j+1:03d}",
                    warehouse=warehouse
                )
                self.test_data['locations'].append(location)
                location_count += 1
        
        print(f"âœ… {location_count} emplacements crÃ©Ã©s")
        
        # Les propriÃ©tÃ©s sont directement sur le modÃ¨le Product (n_lot, n_serie, dlc)
        
        # CrÃ©er des produits (100 produits)
        for i in range(100):
            product = Product.objects.create(
                Internal_Product_Code=f"PROD-{i+1:05d}",
                Short_Description=f"Produit Test {i+1:03d}",
                Barcode=f"BAR-{i+1:08d}",
                Stock_Unit="UnitÃ©",
                Product_Status="ACTIVE",
                Product_Family=None,  # Vous pouvez crÃ©er des familles si nÃ©cessaire
                n_lot=random.choice([True, False]),
                n_serie=random.choice([True, False]),
                dlc=random.choice([True, False])
            )
            self.test_data['products'].append(product)
        
        print(f"âœ… {len(self.test_data['products'])} produits crÃ©Ã©s")
        
        # CrÃ©er des inventaires (10 inventaires)
        for i in range(10):
            inventory = Inventory.objects.create(
                name=f"Inventaire Test {i+1}",
                account=random.choice(self.test_data['accounts']),
                warehouse=random.choice(self.test_data['warehouses'])
            )
            self.test_data['inventories'].append(inventory)
        
        # CrÃ©er des comptages (30 comptages)
        count_modes = ["en vrac", "par article", "image de stock"]
        for i in range(30):
            counting = Counting.objects.create(
                order=i+1,
                count_mode=random.choice(count_modes),
                unit_scanned=random.choice([True, False]),
                entry_quantity=random.choice([True, False]),
                stock_situation=random.choice([True, False]),
                is_variant=random.choice([True, False]),
                n_lot=random.choice([True, False]),
                n_serie=random.choice([True, False]),
                dlc=random.choice([True, False]),
                show_product=random.choice([True, False]),
                quantity_show=random.choice([True, False]),
                inventory=random.choice(self.test_data['inventories'])
            )
            self.test_data['countings'].append(counting)
        
        print(f"âœ… {len(self.test_data['countings'])} comptages crÃ©Ã©s")
        
        # CrÃ©er des jobs et assignments
        for i in range(20):
            job = Job.objects.create(
                inventory=random.choice(self.test_data['inventories'])
            )
            self.test_data['jobs'].append(job)
            
            # CrÃ©er des job details pour ce job
            selected_locations = random.sample(self.test_data['locations'], min(10, len(self.test_data['locations'])))
            for location in selected_locations:
                job_detail = JobDetail.objects.create(
                    location=location,
                    job=job,
                    counting=random.choice(self.test_data['countings']),
                    status='EN ATTENTE'
                )
                self.test_data['job_details'].append(job_detail)
            
            # CrÃ©er un assignment
            assignment = Assigment.objects.create(
                job=job,
                user=self.test_user,
                status='EN COURS'
            )
            self.test_data['assignments'].append(assignment)
        
        print(f"âœ… {len(self.test_data['jobs'])} jobs et {len(self.test_data['assignments'])} assignments crÃ©Ã©s")
        print(f"âœ… {len(self.test_data['job_details'])} job details crÃ©Ã©s")
    
    def _generate_test_data(self, index: int) -> Dict[str, Any]:
        """GÃ©nÃ¨re des donnÃ©es de test pour un CountingDetail."""
        counting = random.choice(self.test_data['countings'])
        location = random.choice(self.test_data['locations'])
        assignment = random.choice(self.test_data['assignments'])
        
        # DonnÃ©es de base
        data = {
            'counting_id': counting.id,
            'location_id': location.id,
            'quantity_inventoried': random.randint(1, 100),
            'assignment_id': assignment.id
        }
        
        # Ajouter product_id selon le mode de comptage
        if counting.count_mode == "par article":
            data['product_id'] = random.choice(self.test_data['products']).id
        elif random.choice([True, False]):  # 50% de chance pour les autres modes
            data['product_id'] = random.choice(self.test_data['products']).id
        
        # Ajouter DLC si nÃ©cessaire
        if counting.dlc or random.choice([True, False]):
            future_date = datetime.now() + timedelta(days=random.randint(30, 365))
            data['dlc'] = future_date.strftime('%Y-%m-%d')
        
        # Ajouter numÃ©ro de lot si nÃ©cessaire
        if counting.n_lot or random.choice([True, False]):
            data['n_lot'] = f"LOT-{index:05d}-{random.randint(1000, 9999)}"
        
        # Ajouter numÃ©ros de sÃ©rie si nÃ©cessaire
        if counting.n_serie and random.choice([True, False]):
            num_series = random.randint(1, min(5, data['quantity_inventoried']))
            data['numeros_serie'] = []
            for j in range(num_series):
                data['numeros_serie'].append({
                    'n_serie': f"NS-{index:05d}-{j+1:03d}-{random.randint(1000, 9999)}"
                })
        
        return data
    
    def _create_counting_detail_api(self, data: Dict[str, Any], index: int) -> Dict[str, Any]:
        """CrÃ©e un CountingDetail via l'API REST."""
        start_time = time.time()
        
        try:
            headers = {}
            if self.test_token:
                headers['Authorization'] = f'Bearer {self.test_token}'
            
            if self.test_token:
                # Utiliser requests pour les appels HTTP
                response = requests.post(
                    f"{self.api_base}/counting-detail/",
                    json=data,
                    headers=headers
                )
                
                response_time = time.time() - start_time
                
                result = {
                    'index': index,
                    'status_code': response.status_code,
                    'response_time': response_time,
                    'success': response.status_code == 201,
                    'data': data
                }
                
                if response.status_code == 201:
                    result['response_data'] = response.json()
                else:
                    result['error'] = response.text
                    
            else:
                # Utiliser le client API Django
                response = self.api_client.post(
                    reverse('mobile:mobile_counting_detail'),
                    data,
                    format='json'
                )
                
                response_time = time.time() - start_time
                
                result = {
                    'index': index,
                    'status_code': response.status_code,
                    'response_time': response_time,
                    'success': response.status_code == 201,
                    'data': data
                }
                
                if response.status_code == 201:
                    result['response_data'] = response.data
                else:
                    result['error'] = str(response.data) if hasattr(response, 'data') else str(response.content)
            
            return result
            
        except Exception as e:
            response_time = time.time() - start_time
            return {
                'index': index,
                'status_code': 500,
                'response_time': response_time,
                'success': False,
                'error': str(e),
                'data': data
            }
    
    def _create_counting_detail_batch(self, start_index: int, count: int) -> List[Dict[str, Any]]:
        """CrÃ©e un lot de CountingDetail."""
        results = []
        
        for i in range(count):
            index = start_index + i
            data = self._generate_test_data(index)
            result = self._create_counting_detail_api(data, index)
            results.append(result)
            
            # Affichage du progrÃ¨s
            if (index + 1) % 50 == 0:
                print(f"  âœ… {index + 1} CountingDetail traitÃ©s")
        
        return results
    
    def test_1000_counting_details_sequential(self):
        """Test sÃ©quentiel de crÃ©ation de 1000 CountingDetail."""
        print("\nğŸš€ Test sÃ©quentiel de 1000 CountingDetail...")
        print("=" * 60)
        
        start_time = time.time()
        all_results = []
        
        # CrÃ©er 1000 CountingDetail de maniÃ¨re sÃ©quentielle
        for i in range(1000):
            data = self._generate_test_data(i)
            result = self._create_counting_detail_api(data, i)
            all_results.append(result)
            
            # Mise Ã  jour des mÃ©triques
            if result['success']:
                self.performance_metrics['success_count'] += 1
                self.performance_metrics['creation_times'].append(result['response_time'])
            else:
                self.performance_metrics['error_count'] += 1
                self.performance_metrics['errors'].append({
                    'index': i,
                    'error': result.get('error', 'Unknown error'),
                    'data': result['data']
                })
            
            self.performance_metrics['response_times'].append(result['response_time'])
            
            # Affichage du progrÃ¨s
            if (i + 1) % 100 == 0:
                print(f"  âœ… {i + 1}/1000 CountingDetail traitÃ©s")
        
        total_time = time.time() - start_time
        
        print(f"\nğŸ“Š RÃ©sultats du test sÃ©quentiel:")
        print(f"  â€¢ Temps total: {total_time:.2f} secondes")
        print(f"  â€¢ SuccÃ¨s: {self.performance_metrics['success_count']}/1000")
        print(f"  â€¢ Erreurs: {self.performance_metrics['error_count']}/1000")
        print(f"  â€¢ Temps moyen par requÃªte: {statistics.mean(self.performance_metrics['response_times']):.3f}s")
        print(f"  â€¢ Temps mÃ©dian par requÃªte: {statistics.median(self.performance_metrics['response_times']):.3f}s")
        
        if self.performance_metrics['creation_times']:
            print(f"  â€¢ Temps moyen de crÃ©ation (succÃ¨s): {statistics.mean(self.performance_metrics['creation_times']):.3f}s")
        
        return all_results
    
    def test_1000_counting_details_parallel(self):
        """Test parallÃ¨le de crÃ©ation de 1000 CountingDetail."""
        print("\nğŸš€ Test parallÃ¨le de 1000 CountingDetail...")
        print("=" * 60)
        
        start_time = time.time()
        all_results = []
        
        # RÃ©initialiser les mÃ©triques
        self.performance_metrics = {
            'creation_times': [],
            'response_times': [],
            'success_count': 0,
            'error_count': 0,
            'errors': []
        }
        
        # Utiliser ThreadPoolExecutor pour les requÃªtes parallÃ¨les
        with ThreadPoolExecutor(max_workers=10) as executor:
            # Diviser en lots de 100
            futures = []
            for batch_start in range(0, 1000, 100):
                batch_size = min(100, 1000 - batch_start)
                future = executor.submit(self._create_counting_detail_batch, batch_start, batch_size)
                futures.append(future)
            
            # Collecter les rÃ©sultats
            for future in as_completed(futures):
                try:
                    batch_results = future.result()
                    all_results.extend(batch_results)
                    
                    # Mise Ã  jour des mÃ©triques
                    for result in batch_results:
                        if result['success']:
                            self.performance_metrics['success_count'] += 1
                            self.performance_metrics['creation_times'].append(result['response_time'])
                        else:
                            self.performance_metrics['error_count'] += 1
                            self.performance_metrics['errors'].append({
                                'index': result['index'],
                                'error': result.get('error', 'Unknown error'),
                                'data': result['data']
                            })
                        
                        self.performance_metrics['response_times'].append(result['response_time'])
                        
                except Exception as e:
                    print(f"âŒ Erreur dans le lot: {e}")
        
        total_time = time.time() - start_time
        
        print(f"\nğŸ“Š RÃ©sultats du test parallÃ¨le:")
        print(f"  â€¢ Temps total: {total_time:.2f} secondes")
        print(f"  â€¢ SuccÃ¨s: {self.performance_metrics['success_count']}/1000")
        print(f"  â€¢ Erreurs: {self.performance_metrics['error_count']}/1000")
        
        if self.performance_metrics['response_times']:
            print(f"  â€¢ Temps moyen par requÃªte: {statistics.mean(self.performance_metrics['response_times']):.3f}s")
            print(f"  â€¢ Temps mÃ©dian par requÃªte: {statistics.median(self.performance_metrics['response_times']):.3f}s")
            print(f"  â€¢ Temps min par requÃªte: {min(self.performance_metrics['response_times']):.3f}s")
            print(f"  â€¢ Temps max par requÃªte: {max(self.performance_metrics['response_times']):.3f}s")
        
        if self.performance_metrics['creation_times']:
            print(f"  â€¢ Temps moyen de crÃ©ation (succÃ¨s): {statistics.mean(self.performance_metrics['creation_times']):.3f}s")
        
        return all_results
    
    def test_api_validation_scenarios(self):
        """Test des diffÃ©rents scÃ©narios de validation."""
        print("\nğŸ§ª Test des scÃ©narios de validation...")
        print("=" * 60)
        
        validation_tests = [
            {
                'name': 'DonnÃ©es manquantes - counting_id',
                'data': {
                    'location_id': self.test_data['locations'][0].id,
                    'quantity_inventoried': 10,
                    'assignment_id': self.test_data['assignments'][0].id
                },
                'expected_status': 400
            },
            {
                'name': 'DonnÃ©es manquantes - location_id',
                'data': {
                    'counting_id': self.test_data['countings'][0].id,
                    'quantity_inventoried': 10,
                    'assignment_id': self.test_data['assignments'][0].id
                },
                'expected_status': 400
            },
            {
                'name': 'QuantitÃ© nÃ©gative',
                'data': {
                    'counting_id': self.test_data['countings'][0].id,
                    'location_id': self.test_data['locations'][0].id,
                    'quantity_inventoried': -5,
                    'assignment_id': self.test_data['assignments'][0].id
                },
                'expected_status': 400
            },
            {
                'name': 'Mode par article sans product_id',
                'data': {
                    'counting_id': next((c.id for c in self.test_data['countings'] if c.count_mode == "par article"), None),
                    'location_id': self.test_data['locations'][0].id,
                    'quantity_inventoried': 10,
                    'assignment_id': self.test_data['assignments'][0].id
                },
                'expected_status': 400
            }
        ]
        
        validation_results = []
        
        for i, test in enumerate(validation_tests):
            if test['data'].get('counting_id') is None:
                print(f"  âš ï¸ Test '{test['name']}' ignorÃ© - pas de comptage appropriÃ©")
                continue
                
            print(f"  ğŸ§ª Test {i+1}: {test['name']}")
            
            result = self._create_counting_detail_api(test['data'], f"validation_{i}")
            
            validation_results.append({
                'test_name': test['name'],
                'expected_status': test['expected_status'],
                'actual_status': result['status_code'],
                'success': result['status_code'] == test['expected_status'],
                'error': result.get('error', '')
            })
            
            if result['status_code'] == test['expected_status']:
                print(f"    âœ… Validation correcte (Status: {result['status_code']})")
            else:
                print(f"    âŒ Validation incorrecte (Attendu: {test['expected_status']}, ReÃ§u: {result['status_code']})")
                print(f"    ğŸ“ Erreur: {result.get('error', 'Aucune erreur')}")
        
        # RÃ©sumÃ© des tests de validation
        successful_validations = sum(1 for r in validation_results if r['success'])
        print(f"\nğŸ“Š RÃ©sultats des tests de validation:")
        print(f"  â€¢ Tests rÃ©ussis: {successful_validations}/{len(validation_results)}")
        
        return validation_results
    
    def test_data_retrieval(self):
        """Test de rÃ©cupÃ©ration des donnÃ©es."""
        print("\nğŸ“¥ Test de rÃ©cupÃ©ration des donnÃ©es...")
        print("=" * 60)
        
        # CrÃ©er quelques CountingDetail pour les tests de rÃ©cupÃ©ration
        print("  ğŸ“¦ CrÃ©ation de donnÃ©es de test pour la rÃ©cupÃ©ration...")
        test_counting = self.test_data['countings'][0]
        test_location = self.test_data['locations'][0]
        test_product = self.test_data['products'][0]
        
        # CrÃ©er 5 CountingDetail pour les tests
        for i in range(5):
            data = {
                'counting_id': test_counting.id,
                'location_id': test_location.id,
                'quantity_inventoried': random.randint(1, 20),
                'assignment_id': self.test_data['assignments'][0].id,
                'product_id': test_product.id
            }
            self._create_counting_detail_api(data, f"retrieval_test_{i}")
        
        retrieval_tests = [
            {
                'name': 'RÃ©cupÃ©ration par counting_id',
                'url': f"{self.api_base}/counting-detail/?counting_id={test_counting.id}"
            },
            {
                'name': 'RÃ©cupÃ©ration par location_id',
                'url': f"{self.api_base}/counting-detail/?location_id={test_location.id}"
            },
            {
                'name': 'RÃ©cupÃ©ration par product_id',
                'url': f"{self.api_base}/counting-detail/?product_id={test_product.id}"
            }
        ]
        
        retrieval_results = []
        
        for test in retrieval_tests:
            print(f"  ğŸ” {test['name']}")
            
            try:
                headers = {}
                if self.test_token:
                    headers['Authorization'] = f'Bearer {self.test_token}'
                
                start_time = time.time()
                
                if self.test_token:
                    response = requests.get(test['url'], headers=headers)
                    response_time = time.time() - start_time
                    
                    result = {
                        'test_name': test['name'],
                        'status_code': response.status_code,
                        'response_time': response_time,
                        'success': response.status_code == 200
                    }
                    
                    if response.status_code == 200:
                        data = response.json()
                        result['count'] = len(data.get('data', {}).get('counting_details', []))
                        print(f"    âœ… {result['count']} Ã©lÃ©ments rÃ©cupÃ©rÃ©s en {response_time:.3f}s")
                    else:
                        result['error'] = response.text
                        print(f"    âŒ Erreur: {response.text}")
                else:
                    # Utiliser le client Django pour les tests
                    if 'counting_id' in test['url']:
                        response = self.api_client.get(
                            reverse('mobile:mobile_counting_detail'),
                            {'counting_id': test_counting.id}
                        )
                    elif 'location_id' in test['url']:
                        response = self.api_client.get(
                            reverse('mobile:mobile_counting_detail'),
                            {'location_id': test_location.id}
                        )
                    elif 'product_id' in test['url']:
                        response = self.api_client.get(
                            reverse('mobile:mobile_counting_detail'),
                            {'product_id': test_product.id}
                        )
                    
                    response_time = time.time() - start_time
                    
                    result = {
                        'test_name': test['name'],
                        'status_code': response.status_code,
                        'response_time': response_time,
                        'success': response.status_code == 200
                    }
                    
                    if response.status_code == 200:
                        result['count'] = len(response.data.get('data', {}).get('counting_details', []))
                        print(f"    âœ… {result['count']} Ã©lÃ©ments rÃ©cupÃ©rÃ©s en {response_time:.3f}s")
                    else:
                        result['error'] = str(response.data) if hasattr(response, 'data') else str(response.content)
                        print(f"    âŒ Erreur: {result['error']}")
                
                retrieval_results.append(result)
                
            except Exception as e:
                print(f"    âŒ Exception: {e}")
                retrieval_results.append({
                    'test_name': test['name'],
                    'status_code': 500,
                    'success': False,
                    'error': str(e)
                })
        
        # RÃ©sumÃ© des tests de rÃ©cupÃ©ration
        successful_retrievals = sum(1 for r in retrieval_results if r['success'])
        print(f"\nğŸ“Š RÃ©sultats des tests de rÃ©cupÃ©ration:")
        print(f"  â€¢ Tests rÃ©ussis: {successful_retrievals}/{len(retrieval_results)}")
        
        return retrieval_results
    
    def generate_performance_report(self):
        """GÃ©nÃ¨re un rapport de performance dÃ©taillÃ©."""
        print("\nğŸ“Š RAPPORT DE PERFORMANCE")
        print("=" * 80)
        
        # Statistiques gÃ©nÃ©rales
        if self.performance_metrics['response_times']:
            response_times = self.performance_metrics['response_times']
            creation_times = self.performance_metrics['creation_times']
            
            print(f"ğŸ¯ STATISTIQUES GÃ‰NÃ‰RALES:")
            print(f"  â€¢ Total de requÃªtes: {len(response_times)}")
            print(f"  â€¢ SuccÃ¨s: {self.performance_metrics['success_count']}")
            print(f"  â€¢ Erreurs: {self.performance_metrics['error_count']}")
            print(f"  â€¢ Taux de succÃ¨s: {(self.performance_metrics['success_count']/len(response_times)*100):.1f}%")
            
            print(f"\nâ±ï¸ TEMPS DE RÃ‰PONSE:")
            print(f"  â€¢ Temps moyen: {statistics.mean(response_times):.3f}s")
            print(f"  â€¢ Temps mÃ©dian: {statistics.median(response_times):.3f}s")
            print(f"  â€¢ Temps minimum: {min(response_times):.3f}s")
            print(f"  â€¢ Temps maximum: {max(response_times):.3f}s")
            print(f"  â€¢ Ã‰cart type: {statistics.stdev(response_times):.3f}s")
            
            if creation_times:
                print(f"\nâœ… TEMPS DE CRÃ‰ATION (SUCCÃˆS UNIQUEMENT):")
                print(f"  â€¢ Temps moyen: {statistics.mean(creation_times):.3f}s")
                print(f"  â€¢ Temps mÃ©dian: {statistics.median(creation_times):.3f}s")
                print(f"  â€¢ Temps minimum: {min(creation_times):.3f}s")
                print(f"  â€¢ Temps maximum: {max(creation_times):.3f}s")
            
            # Analyse des percentiles
            sorted_times = sorted(response_times)
            p50 = sorted_times[int(len(sorted_times) * 0.5)]
            p90 = sorted_times[int(len(sorted_times) * 0.9)]
            p95 = sorted_times[int(len(sorted_times) * 0.95)]
            p99 = sorted_times[int(len(sorted_times) * 0.99)]
            
            print(f"\nğŸ“ˆ PERCENTILES DES TEMPS DE RÃ‰PONSE:")
            print(f"  â€¢ P50 (mÃ©diane): {p50:.3f}s")
            print(f"  â€¢ P90: {p90:.3f}s")
            print(f"  â€¢ P95: {p95:.3f}s")
            print(f"  â€¢ P99: {p99:.3f}s")
        
        # Analyse des erreurs
        if self.performance_metrics['errors']:
            print(f"\nâŒ ANALYSE DES ERREURS:")
            error_types = {}
            for error in self.performance_metrics['errors']:
                error_msg = error['error'][:100]  # Limiter la longueur
                error_types[error_msg] = error_types.get(error_msg, 0) + 1
            
            for error_msg, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):
                print(f"  â€¢ {error_msg}: {count} occurrences")
        
        # Recommandations de performance
        print(f"\nğŸ”§ RECOMMANDATIONS:")
        if self.performance_metrics['response_times']:
            avg_time = statistics.mean(self.performance_metrics['response_times'])
            if avg_time > 1.0:
                print(f"  âš ï¸ Temps de rÃ©ponse Ã©levÃ© ({avg_time:.3f}s). ConsidÃ©rer l'optimisation.")
            elif avg_time > 0.5:
                print(f"  âš¡ Temps de rÃ©ponse acceptable ({avg_time:.3f}s).")
            else:
                print(f"  ğŸš€ Excellent temps de rÃ©ponse ({avg_time:.3f}s)!")
        
        error_rate = self.performance_metrics['error_count'] / max(1, len(self.performance_metrics['response_times'])) * 100
        if error_rate > 10:
            print(f"  âŒ Taux d'erreur Ã©levÃ© ({error_rate:.1f}%). VÃ©rifier la stabilitÃ© de l'API.")
        elif error_rate > 5:
            print(f"  âš ï¸ Taux d'erreur modÃ©rÃ© ({error_rate:.1f}%). Surveiller.")
        else:
            print(f"  âœ… Faible taux d'erreur ({error_rate:.1f}%).")
    
    def cleanup(self):
        """Nettoyage des donnÃ©es de test."""
        print("\nğŸ§¹ Nettoyage des donnÃ©es de test...")
        
        try:
            # Supprimer les donnÃ©es crÃ©Ã©es (dans l'ordre inverse des dÃ©pendances)
            CountingDetail.objects.filter(
                counting__inventory__name__startswith="Inventaire Test"
            ).delete()
            
            NSerieInventory.objects.filter(
                counting_detail__counting__inventory__name__startswith="Inventaire Test"
            ).delete()
            
            for model_list in [
                'job_details', 'assignments', 'jobs', 'countings', 
                'inventories', 'products', 'locations', 'warehouses', 'accounts'
            ]:
                for obj in self.test_data[model_list]:
                    try:
                        obj.delete()
                    except:
                        pass
            
            # Supprimer l'utilisateur de test
            if self.test_user:
                self.test_user.delete()
            
            print("âœ… Nettoyage terminÃ©")
            
        except Exception as e:
            print(f"âš ï¸ Erreur lors du nettoyage: {e}")
    
    def run_full_test_suite(self):
        """ExÃ©cute la suite complÃ¨te de tests."""
        print("ğŸš€ DÃ‰MARRAGE DE LA SUITE DE TESTS COUNTING DETAIL")
        print("=" * 80)
        print(f"ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¯ Objectif: Tester 1000 crÃ©ations de CountingDetail")
        print("=" * 80)
        
        try:
            # Configuration
            self.setUp()
            
            # Tests de validation
            validation_results = self.test_api_validation_scenarios()
            
            # Test de rÃ©cupÃ©ration
            retrieval_results = self.test_data_retrieval()
            
            # Test principal - 1000 CountingDetail (sÃ©quentiel)
            sequential_results = self.test_1000_counting_details_sequential()
            
            # Test principal - 1000 CountingDetail (parallÃ¨le)
            parallel_results = self.test_1000_counting_details_parallel()
            
            # GÃ©nÃ©ration du rapport final
            self.generate_performance_report()
            
            print(f"\nğŸ TESTS TERMINÃ‰S AVEC SUCCÃˆS!")
            print(f"ğŸ“Š RÃ©sumÃ©:")
            print(f"  â€¢ Tests de validation: {len(validation_results)} exÃ©cutÃ©s")
            print(f"  â€¢ Tests de rÃ©cupÃ©ration: {len(retrieval_results)} exÃ©cutÃ©s")
            print(f"  â€¢ Tests de performance: 2000 CountingDetail crÃ©Ã©s")
            print(f"  â€¢ Taux de succÃ¨s global: {(self.performance_metrics['success_count']/max(1, len(self.performance_metrics['response_times']))*100):.1f}%")
            
        except Exception as e:
            print(f"\nâŒ ERREUR FATALE DANS LA SUITE DE TESTS: {e}")
            import traceback
            traceback.print_exc()
            
        finally:
            # Nettoyage
            self.cleanup()


def main():
    """Fonction principale."""
    print("ğŸ§ª TEST UNITAIRE COUNTING DETAIL - 1000 LIGNES")
    print("=" * 80)
    
    # CrÃ©er et exÃ©cuter les tests
    test_suite = CountingDetailPerformanceTest()
    test_suite.run_full_test_suite()


if __name__ == "__main__":
    main()
